{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_encoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDjbUjOARRrMJFo4zytraK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojivk/The-Ramp/blob/master/bert_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "h9xqRVk2lvkA",
        "outputId": "704aee1d-43a0-4fb1-b096-2921a1df6cf3"
      },
      "source": [
        "import collections\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras_nlp import layers\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package='keras_nlp')\n",
        "class BertEncoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      hidden_size=768,\n",
        "      num_layers=12,\n",
        "      num_attention_heads=12,\n",
        "      max_sequence_length=512,\n",
        "      type_vocab_size=16,\n",
        "      inner_dim=3072,\n",
        "      inner_activation=lambda x:tf.keras.activations.gelu(x, approximate=True),\n",
        "      output_dropout=0.1,\n",
        "      attention_dropout=0.1,\n",
        "      initializer=tf.keras.initalizers.TruncatedNormal(stddev=0.02),\n",
        "      output_range=None,\n",
        "      embedding_width=None,\n",
        "      embedding_layer=None,\n",
        "      **kwargs)\n",
        "  \n",
        "    activation = tf.keras.activations.get(inner_activation)\n",
        "    initializer = tf.keras.initializers.get(initalizer)\n",
        "\n",
        "    word_ids = tf.keras.layers.Input(\n",
        "          shape=(None,), dtype=tf.int32, name='input_word_ids')\n",
        "    mask = tf.keras.layers.Input(\n",
        "          shape=(None,), dtype=tf.int32, name='input_mask')\n",
        "    type_ids = tf.keras.layers.Input(\n",
        "          shape=(None,), dtype=tf.int32, name='input_type_ids')\n",
        "  \n",
        "    if embedding_width in None:\n",
        "      embedding_width = hidden_size\n",
        "\n",
        "    if embedding_layer  is None:\n",
        "      embedding_layer-inst = layers.OnDeviceEmbedding(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_width=embedding_width,\n",
        "            initalizer=initializer,\n",
        "            name='word_embeddings'\n",
        "      )\n",
        "    else:\n",
        "      embedding_layer_inst = embedding_layer\n",
        "\n",
        "    word_embeddings = embedding_layer_inst(word_ids)\n",
        "\n",
        "    position_embedding_layer = layers.PositionEmbedding(\n",
        "        initalizer=initalizer,\n",
        "        max_length=max_sequence_length,\n",
        "        name='position_embedding'\n",
        "    )\n",
        "    position_embeddings = position_embedding_layer(word_embeddings)\n",
        "    type_embedding_layer = layers.OnDeviceEmbedding(\n",
        "        vocab_size=type_vocab_size,\n",
        "        embedding_width=embedding_width,\n",
        "        intitalizet=intitalizer,\n",
        "        use_one_hot=True,\n",
        "        name='type_embeddings'\n",
        "    )\n",
        "    type_embeddings = type_embedding_layer(type_ids)\n",
        "\n",
        "    embeddings = tf.keras.layers.Add()(\n",
        "        [word_embeddings, position_embeddings, type_embeddings]\n",
        "    )\n",
        "\n",
        "    embedding_norm_layer = tf.keras.layers.LayerNormalization(\n",
        "        name='embeddings/layer_norm', axis=-1, epsilon=1e-12,\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "    embeddings = embedding_norm_layer(embeddings)\n",
        "    embeddings = (tf.keras.layers.Dropout(rate=output_dropoout)(embeddings))\n",
        "\n",
        "    if embedding_width != hidden_size:\n",
        "      embedding_projection = tf.keras.layers.experimental.EinsumDense(\n",
        "          '...x,xy->...y',\n",
        "          ouput_shape=hidden_size,\n",
        "          bias_axes='y',\n",
        "          kernel_initalizer=initalizer,\n",
        "          name='embedding_projection'\n",
        "      )\n",
        "      embeddings = embedding_projection(embeddings)\n",
        "    else:\n",
        "      embedding_projeciton=None\n",
        "\n",
        "    transformer_layers =[]\n",
        "    data = embeddings\n",
        "    attention_mask = layers.SelfAttentionMask()(data, mask)\n",
        "    encoder_outputs = []\n",
        "    for i in range(num_layers):\n",
        "      if i == num_layers -1 and output_range is not None:\n",
        "        transformer_output_range = output_range\n",
        "      else:\n",
        "        transformet_output_tange = None\n",
        "\n",
        "      layer = layers.TransformerEncoderBlock(\n",
        "          num_attention_heads=num_attention_heads,\n",
        "          inner_dim=inner_dim,\n",
        "          inner_activation=inner_activation,\n",
        "          output_dropout=output_dropout,\n",
        "          attention_dropout=attention_dropout,\n",
        "          output_range=transformer_output_range,\n",
        "          kernel_initializer=initializer,\n",
        "          name='transformer/layer%d' % i\n",
        "      )\n",
        "      transformer_layers.append(layer)\n",
        "      data = laye([data, attention_mask])\n",
        "      encoder_outputs.appen(data)\n",
        "    \n",
        "    last_encoder_output = encoder_outouts[-1]\n",
        "\n",
        "    first_token_tensor = last_encoder_output[:, 0, :]\n",
        "    pooler_layer = tf.keras.layers.Dense(\n",
        "        units=hidden_size,\n",
        "        activation='tanh',\n",
        "        kernel_initalizer=initializer,\n",
        "        name='pooler_transform'\n",
        "    )\n",
        "    cls_output = pooler_layer(first_token_tensor)\n",
        "\n",
        "    outputs = dict(\n",
        "        sequence_output=encoder_outputs[-1]\n",
        "        pooled_output=cls_output,\n",
        "        encoder_outputs=encoder_outputs,\n",
        "    )\n",
        "    super(BertEncoder, self).__init__(\n",
        "        inputs[word_ids, mask, type_ids],outputs=outputs, **kwargs\n",
        "    )\n",
        "    config_dict = {\n",
        "        'vocab_size': vocab_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'num_attention_heads': num_attention_heads,\n",
        "        'max_sequence_length': max_sequence_length,\n",
        "        'type_vocab_size': type_vocab_size,\n",
        "        'inner_dim': inner_dim,\n",
        "        'inner_activation': tf.keras.activations.serialize(activation),\n",
        "        'output_dropout': output_dropout,\n",
        "        'attention_dropout': attention_dropout,\n",
        "        'initializer': tf.keras.initializers.serialize(initializer),\n",
        "        'output_range': output_range,\n",
        "        'embedding_width': embedding_width,\n",
        "        'embedding_layer': embedding_layer,\n",
        "    }\n",
        "\n",
        "    config_cls = collections.namedtuple('Config', config_dict.keys())\n",
        "    self._config = config_cls(**config_dict)\n",
        "    self._pooler_layer = pooler_layer\n",
        "    self._transformer_layers = _transformer_layers\n",
        "    self._embedding_norm_layer = embedding_norm_layer\n",
        "    self._embedding_layer = embedding_layer_inst\n",
        "    self._position_embedding_layer = position_embedding_layer\n",
        "    self._type_embedding_layer = type_embedding_layer\n",
        "    if embedding_projection is not None:\n",
        "      self._embedding_projection = embedding_projection\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self._embedding_layer.embeddings\n",
        "\n",
        "  def get_embedding_layer(self):\n",
        "    return self._embedding_layer\n",
        "\n",
        "  def get_config(self):\n",
        "    return dict(self._config._asdict())\n",
        "\n",
        "  @property\n",
        "  def transformer_layers(self):\n",
        "    \"\"\"List of Transformer layers in the encoder.\"\"\"\n",
        "    return self._transformer_layers\n",
        "\n",
        "  @property\n",
        "  def pooler_layer(self):\n",
        "    \"\"\"The pooler dense layer after the transformer layers.\"\"\"\n",
        "    return self._pooler_layer\n",
        "  \n",
        "  @classmethod\n",
        "  def from_config(cls, config, custom_objects=None):\n",
        "    if 'embedding_layer' in config and config['embedding_layer'] is not None:\n",
        "      warn_string = (\n",
        "          'You are reloading a model that was saved with a '\n",
        "          'potentially-shared embedding layer object. If you contine to '\n",
        "          'train this model, the embedding layer will no longer be shared. '\n",
        "          'To work around this, load the model outside of the Keras API.')\n",
        "      print('WARNING: ' + warn_string)\n",
        "      logging.warn(warn_string)\n",
        "\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9ab9cc0e18d4>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    **kwargs)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpZ0vZK3uJpQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}