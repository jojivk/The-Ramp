{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertmodel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMC+Rq4f7aNUKxB4A7uyXRR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojivk/The-Ramp/blob/master/bertmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8c-F9il-9k_"
      },
      "source": [
        "import gin\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Returns layer that computes custom loss and metrics for pretraining\"\"\"\n",
        "  \n",
        "  def __init__(self, vocab_size, **kwargs):\n",
        "    super(BertPretrainLossAndMetricLayer, self).__init__(**kwargs)\n",
        "    self._vocab_size = vocab_size\n",
        "    self.config = { 'vocab_size':vocab_size,}\n",
        "\n",
        "  def _add_metrics(self, lm_output, lm_labels, lm_label_weights,\n",
        "                   lm_example_loss, sentence_output, sentence_labels,\n",
        "                   next_sentence_loss):\n",
        "    \"\"\"Add metrics\"\"\"\n",
        "    masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(lm_labels,\n",
        "                                                                      lm_output)\n",
        "    numerator = tf.reduce_sum(masked_lm_accuracy * lm_label_weights)\n",
        "    denominator = tf.reduce_sum(lm_label_weights) + 1e-5\n",
        "    masked_lm_accuracy = numerator/denominator\n",
        "    self.add_metric(masked_lm_accuracy, name='masked_lm_accuracy',\n",
        "                    aggregation='mean')\n",
        "    if sentence_labels is not None:\n",
        "      next_sentence_accuracy = tf.keras.metrics.sparse_categorical_accuracy(\n",
        "                               sentence_labesl, sentence_output)\n",
        "      self.add_metric(next_sentence_accuracy, \n",
        "                      name = 'next_sentence_accuracy',\n",
        "                      aggregation='mean')\n",
        "      \n",
        "      if next_sentence_loss is not None:\n",
        "        self.add_metric(next_sentence_loss,\n",
        "                        name='next_sentence_loss',\n",
        "                        aggregation='mean')\n",
        "  \n",
        "  def call(self,\n",
        "           lm_output_logits,\n",
        "           sentence_output_logits,\n",
        "           lm_label_ids,\n",
        "           lm_label_weights,\n",
        "           sentence_labels=None):\n",
        "    \"\"\"Implements call() for the layer\"\"\"\n",
        "    lm_label_weights = tf.cast(lm_label_weights, tf.float32)\n",
        "    lm_output_logits = tf.cast(lm_output_logits, tf.float32)\n",
        "\n",
        "    lm_prediction_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        lm_label_ids, lm_output_logits, from_logits=True)\n",
        "    lm_numerator_loss = tf.reduce_sum(lm_prediction_losses * lm_label_weights)\n",
        "    lm_denominator_loss = tf.reduce_sum(lm_label_weights)\n",
        "    mask_label_loss =tf.math.divide_no_nan(lm_numerator_loss,\n",
        "                                           lm_denominator_loss)\n",
        "    \n",
        "    if sentence_labels is not None:\n",
        "      sentence_output_logits = tf.cast(sentence_output_logits, tf.float32)\n",
        "      sentence_loss =tf.keras.losses.sparse_categorical_crossentropy(\n",
        "          sentence_labels, sentence_output_logits, from_logits=True\n",
        "      )\n",
        "      sentence_loss = tf.reduce_mean(sentence_loss)\n",
        "      loss = mask_label_loss + sentence_loss\n",
        "    else :\n",
        "      sentence_loss = None\n",
        "      loss = mask_label_loss\n",
        "\n",
        "    batch_shape = tf.slice(tf.shape(lm_label_ids), [0], [1])\n",
        "    final_loss = tf.fill(batch_shape, loss)\n",
        "\n",
        "    self._add_metrics(lm_output_logits, lm_label_ids,\n",
        "                      lm_label_weights, mask_label_loss,\n",
        "                      sentence_output_logits, sentence_labels\n",
        "                      sentence_loss)\n",
        "    return final_loss\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "BziNEv-4ZWdg",
        "outputId": "e6600e50-289d-4fe5-ff18-bb6f07de94e7"
      },
      "source": [
        "@gin.configurable\n",
        "def get_transformer_encoder(bert_config,\n",
        "                            transformer_encoder_cls=None,\n",
        "                            output_range=None):\n",
        "  \n",
        "  \"\"\"Gets a transformer encoder object\n",
        "\n",
        "  Args:\n",
        "        bert_config: A 'modeling.BertConfig' or 'modeling.AlbertConfig' object.\n",
        "    sequence_length: [Deprecated].\n",
        "    transformer_encoder_cls: A EncoderScaffold class. If it is None, uses the\n",
        "      default BERT encoder implementation.\n",
        "    output_range: the sequence output range, [0, output_range). Default setting\n",
        "      is to return the entire sequence output.\n",
        "  Returns:\n",
        "    A encoder object.\n",
        "  \"\"\"\n",
        "\n",
        "  if transformer_encoder_cls is not None:\n",
        "    embedding_cfg = dict(\n",
        "        vocab_size=bert-config.vocab_size,\n",
        "        type_vocab_size = bert_config.type_vocab_size,\n",
        "        hidden_size = bert_config.hidden_size,\n",
        "        max_seq_length = bert_config.max_position_embeddings,\n",
        "        inititalizer=tf.keras.inititalizers.TrucatedNormal(\n",
        "            stddev-bert_config.inititalizer_range),\n",
        "            dropput_rate=bert_config.hidden_dropout_prob,\n",
        "    )\n",
        "    hidden_cfg = dict(\n",
        "        num_attention_heads=bert_config.num_attention_heads,\n",
        "        intermediate_size = bert_config.intermediate_size,\n",
        "        intermediate_activation = tf_utils.get_activation(bert_config.hidden_act),\n",
        "        dropout_rate = bert_config.hidden_dropout_prob,\n",
        "        attention_dropout_rate= bert_config.attention_probs_dropout_prob,\n",
        "        kernel_initializer=tf.kers.initializers.TruncatedNormal(\n",
        "            stddev=bert_config.initailizer_range,\n",
        "        )\n",
        "    )\n",
        "    kwargs = dict(\n",
        "        embedding_cfg = enbedding_cfg,\n",
        "        hidden_cfg=hidden_cfg,\n",
        "        num_hidden_instances=bert_config.num_hidden_layers,\n",
        "        pooled_output_dim = bert_config.hidden_size,\n",
        "        pooled_layer_intializer=tf.keras.initializers.TrucatedNormal(\n",
        "            stddev=bert_config.inititalizer_range\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Relies on gin configuration to define the Transformer\n",
        "    # encoder arguments\n",
        "    return transformer_encoder_cls(**kwargs)\n",
        "\n",
        "  kwargs = dict(\n",
        "      vocab_size=bert_config.vocab_size,\n",
        "      hidden_size=bert_config.hidden_size,\n",
        "      num_layers=bert_config.num_hidden_layers,\n",
        "      num_attention_heads=bert_config.num_attention_heads,\n",
        "      intermediate_size=bert_config.intermediate_size,\n",
        "      activation=tf_utils.get_activation(bert_config.hidden_act),\n",
        "      dropout_rate=bert_config.hidden_dropout_prob,\n",
        "      attention_dropout_rate=bert_config.attention_probs_dropout_prob,\n",
        "      max_sequence_length=bert_config.max_position_embeddings,\n",
        "      type_vocab_size=bert_config.type_vocab_size,\n",
        "      embedding_width=bert_config.embedding_size,\n",
        "      initializer=tf.keras.initializers.TruncatedNormal(\n",
        "          stddev=bert_config.initializer_range))\n",
        "  \n",
        "  if isinstance(bert_config, albert_configs.AlbertConfig):\n",
        "    return networks.AlbertEncoder(**kwargs)\n",
        "  else:\n",
        "    assert isinstance(bert_config, configs.BertConfig)\n",
        "    kwargs['output_range'] = output_range\n",
        "    return networks.BertEncoder(**kwargs)\n",
        "\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d71725314cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mgin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigurable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m def get_transformer_encoder(bert_config,\n\u001b[1;32m      3\u001b[0m                             \u001b[0mtransformer_encoder_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             output_range=None):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gin' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C-1dyOUf-Rp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}