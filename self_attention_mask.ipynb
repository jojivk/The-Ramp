{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self_attention_mask.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSggkmU2VH7WJlBRO8/1Yh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojivk/The-Ramp/blob/master/self_attention_mask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za1fOJXGKZyC"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package='keras_nlp')\n",
        "class SelfAttentionMask(tf.keras.layers.Layer):\n",
        "  \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
        "    inputs[0]: from_tensor: 2D or 3D Tensor of shape\n",
        "      [batch_size, from_seq_length, ...].\n",
        "    inputs[1]: to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
        "  \"\"\"\n",
        "\n",
        "  def call(self, inputs, to_mask):\n",
        "    from_shape = tf.shape(inputs)\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "\n",
        "    to_shape = tf.shape(to_mask)\n",
        "    to_seq_length = to_shape[1]\n",
        "\n",
        "    to_mask = tf.cast(\n",
        "        tf.reshape(to_mask, [batch_size, 1, to_seq_length]),\n",
        "        dtype=inputs.dtype)\n",
        "\n",
        "    # We don't assume that `from_tensor` is a mask (although it could be). We\n",
        "    # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
        "    # tokens so we create a tensor of all ones.\n",
        "    #\n",
        "    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
        "    broadcast_ones = tf.ones(\n",
        "        shape=[batch_size, from_seq_length, 1], dtype=inputs.dtype)\n",
        "\n",
        "    # Here we broadcast along two dimensions to create the mask.\n",
        "    mask = broadcast_ones * to_mask\n",
        "\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}