{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "masked_lm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlxUeljH2mIatuQ88f7868",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojivk/The-Ramp/blob/master/masked_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKZNm8U1DT23"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package='keras_nlp')\n",
        "class MaskedLM(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,\n",
        "               embedding_table,\n",
        "               activaton=None,\n",
        "               inititalizer='glorot_uniform',\n",
        "               output='logits',\n",
        "               name=None,\n",
        "               **kwargs):\n",
        "    super(MaskedLM, self)._init__(name=name, **kwargs)\n",
        "    self.embedding_table =embedding_table\n",
        "    self.activation = activation\n",
        "    self.initializer = tf.keras.initalizers.get(initalizer)\n",
        "\n",
        "    if output not in ('predictions', 'logits'):\n",
        "      raise ValueError(\n",
        "          ('Unknown `output` value \"%s\". `output` can be either \"logits\" or '\n",
        "           '\"predictions\"') % output\n",
        "      )\n",
        "      self._output_type = output\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self._vocab_size, hidden_size = self.embedding_table.shape\n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "           hidden_size,\n",
        "           activation=self.activation,\n",
        "           kernel_initalizer=self.initalizer,\n",
        "           name='transform/dense'\n",
        "    )\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization(\n",
        "        axis=-1, epsilon=1e-12, name='transform/layerNorm'\n",
        "    )\n",
        "    self.bias = self.add_weight(\n",
        "        'otuput_bias/bias',\n",
        "        shape=(self._vocab_size,),\n",
        "        initializer='zeros',\n",
        "        trainable=True\n",
        "    )\n",
        "\n",
        "    super(MaskedLM, self).build(input_shape)\n",
        "\n",
        "  def call(self, sequence_data, masked_positions):\n",
        "    masked_lm_input = self._gather_indexes(sequence_data, masked_positions)\n",
        "    lm_data = self.dense(masked_lm_input)\n",
        "    lm_data = self.layer_norm(lm_data)\n",
        "    lm_data = tf.matmul(lm_data, self.embedding_table, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(lm_data, self.bias)\n",
        "    masked_positions_length = masked_positions.shape.as_list()[1] or\n",
        "                       tf.shape(masked_positions)[1]\n",
        "    logits = tf.reshape(logits,\n",
        "                        [-1, masked_positions_length, self._vocab_size])\n",
        "    if self._output_type == 'logits' :\n",
        "      return logits\n",
        "    return tf.nn.log_softmax(logits)\n",
        "\n",
        "  def get_config(self):\n",
        "    raise NotImplementedError('MaskedLM cannot be directly serialized because '\n",
        "                              'it has variable sharing logic.')\n",
        "    \n",
        "  def _gather_indexes(self, sequence_tensor, positions):\n",
        "    \n",
        "    sequence_shape = tf.shape(sequence_tensor)\n",
        "    batch_size, seq_length = sequence_shape[0], sequence_shape[1]\n",
        "    width = sequence_tensor.shape.as_list()[2] or sequence_shape[2]\n",
        "\n",
        "    flat_offsets = tf.reshape(\n",
        "        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
        "    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
        "    flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
        "                                      [batch_size * seq_length, width])\n",
        "    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
        "\n",
        "    return output_tensor\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}