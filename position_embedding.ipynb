{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "position_embedding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwq4wcsxP/I41T0t+sZflU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojivk/The-Ramp/blob/master/position_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HhdjuQQKIJe"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"keras_nlp\")\n",
        "class PositionEmbedding(tf.keras.layers.Layer):\n",
        "  \"\"\"Creates a positional embedding.\n",
        "  Example:\n",
        "  ```python\n",
        "  position_embedding = PositionEmbedding(max_length=100)\n",
        "  inputs = tf.keras.Input((100, 32), dtype=tf.float32)\n",
        "  outputs = position_embedding(inputs)\n",
        "  ```\n",
        "  Args:\n",
        "    max_length: The maximum size of the dynamic sequence.\n",
        "    initializer: The initializer to use for the embedding weights. Defaults to\n",
        "      \"glorot_uniform\".\n",
        "  Reference: This layer creates a positional embedding as described in\n",
        "  [BERT: Pre-training of Deep Bidirectional Transformers for Language\n",
        "  Understanding](https://arxiv.org/abs/1810.04805).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               max_length,\n",
        "               initializer=\"glorot_uniform\",\n",
        "               **kwargs):\n",
        "\n",
        "    super(PositionEmbedding, self).__init__(**kwargs)\n",
        "    if max_length is None:\n",
        "      raise ValueError(\n",
        "          \"`max_length` must be an Integer, not `None`.\"\n",
        "      )\n",
        "    self._max_length = max_length\n",
        "    self._initializer = tf.keras.initializers.get(initializer)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        \"max_length\": self._max_length,\n",
        "        \"initializer\": tf.keras.initializers.serialize(self._initializer),\n",
        "    }\n",
        "    base_config = super(PositionEmbedding, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    dimension_list = input_shape.as_list()\n",
        "\n",
        "    if len(dimension_list) != 3:\n",
        "      raise ValueError(\"PositionEmbedding expects a 3-dimensional input tensor \"\n",
        "                       \"of shape [batch, sequence, width], got \"\n",
        "                       \"{}\".format(input_shape))\n",
        "    seq_length = dimension_list[1]\n",
        "    width = dimension_list[2]\n",
        "\n",
        "    if self._max_length is not None:\n",
        "      weight_sequence_length = self._max_length\n",
        "    else:\n",
        "      weight_sequence_length = seq_length\n",
        "\n",
        "    self._position_embeddings = self.add_weight(\n",
        "        \"embeddings\",\n",
        "        shape=[weight_sequence_length, width],\n",
        "        initializer=self._initializer)\n",
        "\n",
        "    super(PositionEmbedding, self).build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    position_embeddings = self._position_embeddings[:input_shape[1], :]\n",
        "    return tf.broadcast_to(position_embeddings, input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}